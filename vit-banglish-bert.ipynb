{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-11-24T16:51:18.794661Z",
     "iopub.status.busy": "2025-11-24T16:51:18.794003Z",
     "iopub.status.idle": "2025-11-24T16:51:22.502539Z",
     "shell.execute_reply": "2025-11-24T16:51:22.501436Z",
     "shell.execute_reply.started": "2025-11-24T16:51:18.794637Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.53.3)\n",
      "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
      "Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (0.21.0+cu124)\n",
      "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.2.2)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.3)\n",
      "Requirement already satisfied: pillow in /usr/local/lib/python3.11/dist-packages (11.3.0)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (4.67.1)\n",
      "Requirement already satisfied: openpyxl in /usr/local/lib/python3.11/dist-packages (3.1.5)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.20.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.36.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2025.11.3)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.5)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.2)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.15.0)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.5)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.10.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.5.8)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch) (11.2.1.3)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch) (10.3.5.147)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch) (11.6.1.9)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch) (12.3.1.170)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.15.3)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.5.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.6.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: et-xmlfile in /usr/local/lib/python3.11/dist-packages (from openpyxl) (2.0.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (1.2.0)\n",
      "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (1.3.8)\n",
      "Requirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (1.2.4)\n",
      "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (0.1.1)\n",
      "Requirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (2025.3.0)\n",
      "Requirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (2022.3.0)\n",
      "Requirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (2.4.1)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.3)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.10.5)\n",
      "Requirement already satisfied: onemkl-license==2025.3.0 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->transformers) (2025.3.0)\n",
      "Requirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->transformers) (2024.2.0)\n",
      "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->transformers) (2022.3.0)\n",
      "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->transformers) (1.4.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.17->transformers) (2024.2.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.17->transformers) (2024.2.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers torch torchvision scikit-learn pandas pillow tqdm openpyxl\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-24T16:51:22.504843Z",
     "iopub.status.busy": "2025-11-24T16:51:22.504522Z",
     "iopub.status.idle": "2025-11-24T16:51:25.878723Z",
     "shell.execute_reply": "2025-11-24T16:51:25.877685Z",
     "shell.execute_reply.started": "2025-11-24T16:51:22.504809Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: ftfy in /usr/local/lib/python3.11/dist-packages (6.3.1)\n",
      "Requirement already satisfied: wcwidth in /usr/local/lib/python3.11/dist-packages (from ftfy) (0.2.13)\n"
     ]
    }
   ],
   "source": [
    "!pip install ftfy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-11-24T17:32:26.471Z",
     "iopub.execute_input": "2025-11-24T16:51:25.880455Z",
     "iopub.status.busy": "2025-11-24T16:51:25.880124Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n",
      "Found 4 orphan image files not in metadata:\n",
      "  - FB_IMG_1751540473613.jpg\n",
      "  - FB_IMG_1751739942837.jpg\n",
      "  - FB_IMG_1754929300743.jpg\n",
      "  - FB_IMG_1755921270397.jpg\n",
      "Label mapping: {0: 0, 1: 1, 3: 2}\n",
      "Train / Val / Test sizes: 5508 612 680\n",
      "Text normalization enabled\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████| 345/345 [06:38<00:00,  1.16s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.8855725892142446\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Eval: 100%|██████████| 39/39 [00:25<00:00,  1.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Acc: 0.6258169934640523\n",
      "Validation report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7833    0.2733    0.4052       172\n",
      "           1     0.4434    0.7121    0.5465       132\n",
      "           2     0.7118    0.7857    0.7469       308\n",
      "\n",
      "    accuracy                         0.6258       612\n",
      "   macro avg     0.6462    0.5904    0.5662       612\n",
      "weighted avg     0.6740    0.6258    0.6076       612\n",
      "\n",
      "Saved best model.\n",
      "Epoch 2/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████| 345/345 [06:43<00:00,  1.17s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.48130913288721416\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Eval: 100%|██████████| 39/39 [00:23<00:00,  1.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Acc: 0.5800653594771242\n",
      "Validation report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7763    0.3430    0.4758       172\n",
      "           1     0.3822    0.7500    0.5064       132\n",
      "           2     0.7112    0.6396    0.6735       308\n",
      "\n",
      "    accuracy                         0.5801       612\n",
      "   macro avg     0.6232    0.5775    0.5519       612\n",
      "weighted avg     0.6585    0.5801    0.5819       612\n",
      "\n",
      "Epoch 3/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████| 345/345 [06:41<00:00,  1.16s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.24895207187297472\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Eval: 100%|██████████| 39/39 [00:23<00:00,  1.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Acc: 0.6617647058823529\n",
      "Validation report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0     0.5976    0.5698    0.5833       172\n",
      "           1     0.6623    0.3864    0.4880       132\n",
      "           2     0.6900    0.8312    0.7541       308\n",
      "\n",
      "    accuracy                         0.6618       612\n",
      "   macro avg     0.6500    0.5958    0.6085       612\n",
      "weighted avg     0.6581    0.6618    0.6487       612\n",
      "\n",
      "Saved best model.\n",
      "Epoch 4/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████| 345/345 [06:40<00:00,  1.16s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.13539565882132204\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Eval:  77%|███████▋  | 30/39 [00:18<00:05,  1.61it/s]"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"/kaggle/input/dataset6\")\n",
    "\n",
    "import os\n",
    "import argparse\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\n",
    "import torchvision.transforms as T\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from tqdm import tqdm\n",
    "from normalizer import normalize\n",
    "\n",
    "\n",
    "class MemeDataset(Dataset):\n",
    "    def __init__(self, df, images_dir, tokenizer, max_length=128, image_size=224, use_normalizer=True):\n",
    "        self.df = df.reset_index(drop=True)\n",
    "        self.images_dir = Path(images_dir)\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        self.image_size = image_size\n",
    "        self.use_normalizer = use_normalizer\n",
    "\n",
    "        \n",
    "        self.transform = T.Compose([\n",
    "            T.Resize((image_size, image_size)),\n",
    "            T.ToTensor(),  # produces float in [0,1]\n",
    "            T.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                        std=[0.229, 0.224, 0.225])\n",
    "        ])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.loc[idx]\n",
    "        img_path = self.images_dir / row['image_file_name']\n",
    "        try:\n",
    "            img = Image.open(img_path).convert('RGB')\n",
    "        except Exception:\n",
    "            img = Image.new('RGB', (self.image_size, self.image_size), color=(0, 0, 0))\n",
    "        img = self.transform(img)  # tensor [C,H,W], normalized\n",
    "\n",
    "        text = str(row['text']) if pd.notna(row['text']) else \"\"\n",
    "\n",
    "        # Normalize text using the normalizer\n",
    "        if self.use_normalizer and text:\n",
    "            try:\n",
    "                text = normalize(text)\n",
    "            except Exception as e:\n",
    "                print(f\"Warning: Normalization failed for text at index {idx}: {e}\")\n",
    "                \n",
    "                pass\n",
    "\n",
    "        tok = self.tokenizer(text, truncation=True, padding='max_length',\n",
    "                             max_length=self.max_length, return_tensors='pt')\n",
    "        input_ids = tok['input_ids'].squeeze(0)\n",
    "        attention_mask = tok['attention_mask'].squeeze(0)\n",
    "\n",
    "        label = int(row['label'])\n",
    "        return {\n",
    "            'image': img,  \n",
    "            'input_ids': input_ids,\n",
    "            'attention_mask': attention_mask,\n",
    "            'label': torch.tensor(label, dtype=torch.long)\n",
    "        }\n",
    "\n",
    "\n",
    "class MultimodalClassifier(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        text_model_name='csebuetnlp/banglishbert',\n",
    "        num_labels=3,\n",
    "        text_feat_dim=768,\n",
    "        hidden_dim=512,\n",
    "        dropout=0.2,\n",
    "        freeze_text=False,\n",
    "        freeze_image=False\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        # TEXT ENCODER \n",
    "        self.text_encoder = AutoModel.from_pretrained(text_model_name)\n",
    "        if freeze_text:\n",
    "            for p in self.text_encoder.parameters():\n",
    "                p.requires_grad = False\n",
    "\n",
    "        bert_hidden = self.text_encoder.config.hidden_size\n",
    "        self.text_proj = nn.Linear(bert_hidden, text_feat_dim)\n",
    "\n",
    "        #  IMAGE ENCODER (ViT) \n",
    "        \n",
    "        from transformers import ViTModel\n",
    "        self.image_encoder = ViTModel.from_pretrained(\"google/vit-base-patch16-224\")\n",
    "        if freeze_image:\n",
    "            for p in self.image_encoder.parameters():\n",
    "                p.requires_grad = False\n",
    "\n",
    "        image_feat_dim = self.image_encoder.config.hidden_size  \n",
    "        self.image_proj = nn.Linear(image_feat_dim, image_feat_dim)\n",
    "\n",
    "        # CLASSIFIER \n",
    "        concat_dim = text_feat_dim + image_feat_dim\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(concat_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim, num_labels),\n",
    "        )\n",
    "\n",
    "    def forward(self, images, input_ids, attention_mask):\n",
    "        \"\"\"\n",
    "        images: Tensor shape (B, 3, H, W), already normalized to ImageNet mean/std\n",
    "        input_ids, attention_mask: tensors on the same device\n",
    "        \"\"\"\n",
    "        device = next(self.parameters()).device\n",
    "        \n",
    "        images = images.to(device)\n",
    "        input_ids = input_ids.to(device)\n",
    "        attention_mask = attention_mask.to(device)\n",
    "\n",
    "        # IMAGE FORWARD \n",
    "        # ViTModel expects \"pixel_values\" shaped (B, C, H, W)\n",
    "        img_out = self.image_encoder(pixel_values=images)\n",
    "        image_feat = img_out.last_hidden_state[:, 0, :]  # CLS token\n",
    "        image_feat = self.image_proj(image_feat)\n",
    "\n",
    "        #  TEXT FORWARD \n",
    "        txt_out = self.text_encoder(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask\n",
    "        )\n",
    "        text_feat = txt_out.last_hidden_state[:, 0, :]  # CLS token\n",
    "        text_feat = self.text_proj(text_feat)\n",
    "\n",
    "        # FUSION \n",
    "        fusion = torch.cat([image_feat, text_feat], dim=1)\n",
    "        logits = self.classifier(fusion)\n",
    "\n",
    "        return logits\n",
    "\n",
    "\n",
    "def find_discrepancies(df, images_dir):\n",
    "    images_dir = Path(images_dir)\n",
    "    referenced = set(df['image_file_name'].astype(str).tolist())\n",
    "    actual = set([p.name for p in images_dir.glob('*') if p.is_file()])\n",
    "    missing = sorted(list(referenced - actual))\n",
    "    orphan = sorted(list(actual - referenced))\n",
    "    return missing, orphan\n",
    "\n",
    "\n",
    "def prepare_dataframe(path, images_dir, drop_label_value=2):\n",
    "    df = pd.read_excel(path)\n",
    "    assert 'image_file_name' in df.columns and 'text' in df.columns and 'label' in df.columns, \\\n",
    "        \"metadata.xlsx must contain columns: image_file_name, text, label\"\n",
    "\n",
    "    df = df[df['label'] != drop_label_value].copy()\n",
    "    df['image_file_name'] = df['image_file_name'].astype(str).str.strip()\n",
    "\n",
    "    missing, orphan = find_discrepancies(df, images_dir)\n",
    "    if missing:\n",
    "        print(f\"Missing images for {len(missing)} metadata entries\")\n",
    "        df = df[~df['image_file_name'].isin(missing)].copy()\n",
    "\n",
    "    if orphan:\n",
    "        print(f\"Found {len(orphan)} orphan image files not in metadata:\")\n",
    "        for o in orphan[:20]:\n",
    "            print(\"  -\", o)\n",
    "        if len(orphan) > 20:\n",
    "            print(\"  ... and more\")\n",
    "\n",
    "    unique_labels = sorted(df['label'].unique().tolist())\n",
    "    label_map = {orig: idx for idx, orig in enumerate(unique_labels)}\n",
    "    df['label'] = df['label'].map(label_map)\n",
    "    print(\"Label mapping:\", label_map)\n",
    "    return df, orphan, label_map\n",
    "\n",
    "\n",
    "def compute_class_weights(df):\n",
    "    counts = df['label'].value_counts().sort_index().values\n",
    "    weights = 1.0 / counts\n",
    "    # map label (already remapped to 0..K-1) to its sample weight\n",
    "    sample_weights = df['label'].map(lambda x: weights[x]).values\n",
    "    return sample_weights\n",
    "\n",
    "\n",
    "def collate_fn(batch):\n",
    "    # each b['image'] is a tensor [3,H,W] same H,W by our transform\n",
    "    images = torch.stack([b['image'] for b in batch])\n",
    "    input_ids = torch.stack([b['input_ids'] for b in batch])\n",
    "    attention_mask = torch.stack([b['attention_mask'] for b in batch])\n",
    "    labels = torch.stack([b['label'] for b in batch])\n",
    "    return {\n",
    "        'image': images,\n",
    "        'input_ids': input_ids,\n",
    "        'attention_mask': attention_mask,\n",
    "        'labels': labels\n",
    "    }\n",
    "\n",
    "\n",
    "def train_one_epoch(model, dataloader, optimizer, device):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    for batch in tqdm(dataloader, desc=\"Train\"):\n",
    "        images = batch['image'].to(device)\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        logits = model(images, input_ids, attention_mask)\n",
    "        loss = criterion(logits, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item() * images.size(0)\n",
    "    return total_loss / len(dataloader.dataset)\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(model, dataloader, device, label_map):\n",
    "    model.eval()\n",
    "    preds = []\n",
    "    trues = []\n",
    "    for batch in tqdm(dataloader, desc=\"Eval\"):\n",
    "        images = batch['image'].to(device)\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "\n",
    "        logits = model(images, input_ids, attention_mask)\n",
    "        batch_preds = torch.argmax(logits, dim=1).cpu().numpy().tolist()\n",
    "        batch_trues = labels.cpu().numpy().tolist()\n",
    "        preds.extend(batch_preds)\n",
    "        trues.extend(batch_trues)\n",
    "\n",
    "    acc = accuracy_score(trues, preds)\n",
    "    report = classification_report(trues, preds, digits=4)\n",
    "    return acc, report, trues, preds\n",
    "\n",
    "\n",
    "def main(args):\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(\"Device:\", device)\n",
    "\n",
    "    df, orphan_files, label_map = prepare_dataframe(args.data, args.images_dir, drop_label_value=2)\n",
    "\n",
    "    if args.delete_orphans and orphan_files:\n",
    "        for fname in orphan_files:\n",
    "            p = Path(args.images_dir) / fname\n",
    "            try:\n",
    "                p.unlink()\n",
    "            except Exception as e:\n",
    "                print(\"Could not delete:\", p, e)\n",
    "        print(\"Deleted orphans.\")\n",
    "\n",
    "    train_df, test_df = train_test_split(df, test_size=args.test_size, stratify=df['label'], random_state=42)\n",
    "    train_df, val_df = train_test_split(train_df, test_size=args.val_size, stratify=train_df['label'], random_state=42)\n",
    "    print(\"Train / Val / Test sizes:\", len(train_df), len(val_df), len(test_df))\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(args.text_model)\n",
    "\n",
    "    use_normalizer = not args.disable_normalizer\n",
    "    if use_normalizer:\n",
    "        print(\"Text normalization enabled\")\n",
    "    else:\n",
    "        print(\"Text normalization disabled\")\n",
    "\n",
    "    train_dataset = MemeDataset(train_df, args.images_dir, tokenizer,\n",
    "                                max_length=args.max_length, image_size=args.image_size,\n",
    "                                use_normalizer=use_normalizer)\n",
    "    val_dataset = MemeDataset(val_df, args.images_dir, tokenizer,\n",
    "                              max_length=args.max_length, image_size=args.image_size,\n",
    "                              use_normalizer=use_normalizer)\n",
    "    test_dataset = MemeDataset(test_df, args.images_dir, tokenizer,\n",
    "                               max_length=args.max_length, image_size=args.image_size,\n",
    "                               use_normalizer=use_normalizer)\n",
    "\n",
    "    sample_weights = compute_class_weights(train_df)\n",
    "    sampler = WeightedRandomSampler(sample_weights, num_samples=len(train_dataset), replacement=True)\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=args.batch_size, sampler=sampler, collate_fn=collate_fn)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=args.batch_size, shuffle=False, collate_fn=collate_fn)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=args.batch_size, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "    num_labels = len(label_map)\n",
    "    model = MultimodalClassifier(text_model_name=args.text_model,\n",
    "                                 num_labels=num_labels,\n",
    "                                 text_feat_dim=args.text_feat_dim,\n",
    "                                 hidden_dim=args.hidden_dim,\n",
    "                                 dropout=args.dropout,\n",
    "                                 freeze_text=args.freeze_text,\n",
    "                                 freeze_image=args.freeze_image)\n",
    "    model.to(device)\n",
    "\n",
    "    optimizer = torch.optim.AdamW(filter(lambda p: p.requires_grad, model.parameters()), lr=args.lr)\n",
    "\n",
    "    best_val_acc = 0.0\n",
    "    os.makedirs(args.out_dir, exist_ok=True)\n",
    "\n",
    "    for epoch in range(1, args.epochs + 1):\n",
    "        print(f\"Epoch {epoch}/{args.epochs}\")\n",
    "        train_loss = train_one_epoch(model, train_loader, optimizer, device)\n",
    "        print(\"Train loss:\", train_loss)\n",
    "        val_acc, val_report, _, _ = evaluate(model, val_loader, device, label_map)\n",
    "        print(\"Validation Acc:\", val_acc)\n",
    "        print(\"Validation report:\\n\", val_report)\n",
    "\n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            torch.save({'model_state_dict': model.state_dict(), 'label_map': label_map},\n",
    "                       os.path.join(args.out_dir, \"best_model.pt\"))\n",
    "            print(\"Saved best model.\")\n",
    "\n",
    "    print(\"Testing best model …\")\n",
    "    ckpt = torch.load(os.path.join(args.out_dir, \"best_model.pt\"), map_location=device)\n",
    "    model.load_state_dict(ckpt['model_state_dict'])\n",
    "\n",
    "    test_acc, test_report, trues, preds = evaluate(model, test_loader, device, label_map)\n",
    "    print(\"Test Acc:\", test_acc)\n",
    "    print(\"Test report:\\n\", test_report)\n",
    "\n",
    "    out = test_df.reset_index(drop=True).copy()\n",
    "    out['pred_idx'] = preds\n",
    "    inv_map = {v: k for k, v in label_map.items()}\n",
    "    out['pred_orig'] = out['pred_idx'].map(inv_map)\n",
    "    out.to_csv(os.path.join(args.out_dir, \"test_predictions.csv\"), index=False)\n",
    "    print(\"Saved test predictions to\", os.path.join(args.out_dir, \"test_predictions.csv\"))\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--data', type=str, default='/kaggle/input/dataset6/metadata.xlsx')\n",
    "    parser.add_argument('--images_dir', type=str, default='/kaggle/input/dataset6/images')\n",
    "    parser.add_argument('--out_dir', type=str, default='/kaggle/working/output')\n",
    "    parser.add_argument('--epochs', type=int, default=10)\n",
    "    parser.add_argument('--batch_size', type=int, default=16)\n",
    "    parser.add_argument('--lr', type=float, default=2e-5)\n",
    "    parser.add_argument('--text_model', type=str, default='csebuetnlp/banglishbert')\n",
    "    parser.add_argument('--max_length', type=int, default=64)\n",
    "    parser.add_argument('--image_size', type=int, default=224)\n",
    "    parser.add_argument('--val_size', type=float, default=0.1)\n",
    "    parser.add_argument('--test_size', type=float, default=0.1)\n",
    "    parser.add_argument('--text_feat_dim', type=int, default=768)  # match BERT hidden\n",
    "    parser.add_argument('--hidden_dim', type=int, default=512)\n",
    "    parser.add_argument('--dropout', type=float, default=0.2)\n",
    "    parser.add_argument('--freeze_text', action='store_true')\n",
    "    parser.add_argument('--freeze_image', action='store_true')\n",
    "    parser.add_argument('--delete-orphans', action='store_true')\n",
    "    parser.add_argument('--disable-normalizer', action='store_true',\n",
    "                        help='Disable text normalization (enabled by default)')\n",
    "\n",
    "    args = parser.parse_args([])\n",
    "    main(args)\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 8806930,
     "sourceId": 13828485,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31193,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
